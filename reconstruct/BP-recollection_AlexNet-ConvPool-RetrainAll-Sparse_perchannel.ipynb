{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140cc1d6",
   "metadata": {},
   "source": [
    "<h1><center> BP reconstruction - Alexnet retrained with ConvPool instead of Pool </center></h1>\n",
    "<h2><center> Retrain all AlexNet layers </center></h2>\n",
    "\n",
    "<h3><center> Vary \"sparsity\" </center></h3>\n",
    "\n",
    "### Main conclusion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bf1e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4690ce2-18d1-459c-a207-eb28678e9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/modestyachts/ImageNetV2_pytorch#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "46658a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as F2\n",
    "torch.cuda.empty_cache()\n",
    "#device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "output_images_dir = 'output_images_convPool_retrainAll' # Images for the replacement of pool by strided conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5cbf8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget --no-check-certificate https://leslietj.github.io/image/guided_backpropagation_6.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6608b91",
   "metadata": {},
   "source": [
    "## Download MNIST \n",
    "and take 1 random image from each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7adae27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# Define the data transform to normalize the image data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # normalize the image data\n",
    "])\n",
    "# # Download the MNIST dataset and apply the data transform\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a dictionary to store one random image from each class\n",
    "random_mnist_images = {}\n",
    "# Loop through the dataset to find one random image from each class\n",
    "for i in range(len(trainset)):\n",
    "    image, label = trainset[i]\n",
    "    if label not in random_mnist_images:\n",
    "        random_mnist_images[label] = image\n",
    "        if len(random_mnist_images) == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "146c0328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([5, 0, 4, 1, 9, 2, 3, 6, 7, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_mnist_images.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd8f50-d4fb-410d-87ae-a114a3f7e8f9",
   "metadata": {},
   "source": [
    "## `Modified AlexNet with striedConv instead of Pool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3515a9f9-9b00-4760-b38f-7b467ffab11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load and modify the AlexNet model (as per the previous code)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "def replace_pool_with_conv(model):\n",
    "    for i, layer in enumerate(model.features):\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            # Find the last conv layer before this pooling layer\n",
    "            j = i - 1\n",
    "            while not isinstance(model.features[j], nn.Conv2d):\n",
    "                j -= 1\n",
    "\n",
    "            # Get the number of output channels from the last conv layer\n",
    "            out_channels = model.features[j].out_channels\n",
    "\n",
    "            # Create a new conv layer to replace the pooling layer\n",
    "            conv = nn.Conv2d(in_channels=out_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=3, stride=2, padding=0)\n",
    "            model.features[i] = conv\n",
    "    return model\n",
    "\n",
    "modified_alexnet = replace_pool_with_conv(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97e125e7-4baf-4305-ba51-795e16d8bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "1 ReLU(inplace=True)\n",
      "2 Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "3 Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "4 ReLU(inplace=True)\n",
      "5 Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))\n",
      "6 Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "7 ReLU(inplace=True)\n",
      "8 Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "9 ReLU(inplace=True)\n",
      "10 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "11 ReLU(inplace=True)\n",
      "12 Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n"
     ]
    }
   ],
   "source": [
    "for i, child in enumerate(modified_alexnet.features):\n",
    "    print(i,child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402c989-ea19-4091-b7e1-54a0054de552",
   "metadata": {},
   "source": [
    "## `Re-train on ImageNetV2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "457d6197-3599-4a88-8e4d-f6d910d2c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/modestyachts/ImageNetV2_pytorch\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imagenetv2_pytorch import ImageNetV2Dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "import os, warnings\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(val_loader)\n",
    "def load_model(model_path, model_architecture):\n",
    "    model_architecture.load_state_dict(torch.load(model_path,map_location = device))\n",
    "    model_architecture.to(device)\n",
    "    return model_architecture\n",
    "#############################################\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PROXIES = {'https': 'http://localhost:3166', 'http': 'http://localhost:3166'}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load ImageNetV2 dataset\n",
    "dataset = ImageNetV2Dataset(\"matched-frequency\",transform=transform)  # choose the variant you need\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "# Split dataset into training and validation subsets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training and validation subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "#<strainloader = torch.utils.data.DataLoader(dataset_subset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392423a-efb5-4303-9476-f08f19c20c1d",
   "metadata": {},
   "source": [
    "### `retrain all layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8cd49d77-8653-4c56-bdee-e569276564bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=0 #Do not train the model, load it, change to one if you train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4f86ac9-ddd9-4110-b2e3-7451d9d6275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train == 0:\n",
    "    modified_alexnet = load_model('best_model_train_all_layers.pt', modified_alexnet)\n",
    "else:\n",
    "    # Define the optimizer and scheduler\n",
    "    optimizer = optim.SGD(modified_alexnet.parameters(), lr=0.00001, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    modified_alexnet.to(device)\n",
    "    ############# To check ####################\n",
    "    # Initially, unfreeze only the newly added/modified convolutional layers (assuming these are layers 2, 4, 8 in 'features')\n",
    "    layers_to_unfreeze_initially = [2, 5, 12] #correspond to ex Pooling layers 2,4, 8\n",
    "    for i, child in enumerate(modified_alexnet.features):\n",
    "        if i in layers_to_unfreeze_initially:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Gradual Unfreezing: Define the layers (indices in 'features') to unfreeze over epochs\n",
    "    unfreeze_schedule = { #3,6,8,10\n",
    "        4: [0, 3, 6],\n",
    "        6: [8, 10],\n",
    "        10: 'classifier'  # Unfreeze the fully connected layers\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch in unfreeze_schedule:\n",
    "            layers_to_unfreeze = unfreeze_schedule[epoch]\n",
    "            if layers_to_unfreeze == 'classifier':\n",
    "                # Unfreeze the fully connected layers\n",
    "                for param in modified_alexnet.classifier.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                # Unfreeze specified layers in 'features'\n",
    "                for i in layers_to_unfreeze:\n",
    "                    for param in modified_alexnet.features[i].parameters():\n",
    "                        param.requires_grad = True\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = modified_alexnet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = evaluate(modified_alexnet, val_loader, criterion)\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0  # Reset counter\n",
    "            # Save the best model\n",
    "            torch.save(modified_alexnet.state_dict(), 'best_model_train_all_layers.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:# Early stopping\n",
    "                print(f'Validation loss did not improve for {patience} epochs. Stopping early.')\n",
    "                break\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97a889e2-bcab-4974-be1c-6d3254efb543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the modified AlexNet model\n",
    "torch.save(modified_alexnet.state_dict(), 'modified_alexnet.pth')\n",
    "print('Model Saved Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e4466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epsilon=1e-8\n",
    "large_n = 1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2185b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_input_dense_simple(weights, activations, take_positive = 0):\n",
    "    \"\"\"\n",
    "    Use a mask instead in order to find this problem\n",
    "    Args: \n",
    "        - weights: of shape (N_dim_output, Dim_prev_hidden)\n",
    "        - activations: of shape (1,N_dim_output)\n",
    "    Output:\n",
    "        - reconstructed_input : top weights * corresponding activations\n",
    "    \"\"\"\n",
    "    if take_positive:\n",
    "        activations = torch.clamp(activations, min =0.0) \n",
    "    reconstructed_input = torch.matmul(activations,weights)\n",
    "    return reconstructed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dea89690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_input_conv2D_correct(activations, weights, stride=(1,1), padding=(0,0), does_winner_take_all= 0):\n",
    "    \"\"\"\n",
    "    This function needs to be tested, to be sure it does what is expected\n",
    "    Args: \n",
    "        - weights: of shape (C_out, C_in, kH, kW)\n",
    "        - activations: of shape (B, C_out, H_out, W_out)\n",
    "        - stride: default is (1,1)\n",
    "        - padding: default is (0,0)\n",
    "    Output:\n",
    "        - reconstructed_input : An approximation of backpropagation from selected activations back to the input regions that lead to them\n",
    "    \"\"\"\n",
    "    # get the batch size, number of output channels, out height, and out width\n",
    "    B, C_out, H_out, W_out = activations.shape\n",
    "    C_out, C_in_, kH, kW = weights.shape\n",
    "    # calculate the input height and width\n",
    "    H = (H_out - 1) * stride[0] + kH - 2 * padding[0]\n",
    "    W = (W_out - 1) * stride[1] + kW - 2 * padding[1]\n",
    "    # initialize the input tensor to reconstruct with zeros\n",
    "    reconstructed_input = torch.zeros(B, C_in_, H, W).to(device)\n",
    "    reconstructed_input = F.pad(reconstructed_input, (padding[1], padding[1], padding[0], padding[0]))\n",
    "    if does_winner_take_all == 1:\n",
    "        activations = winner_takes_all(activations)\n",
    "    # loop through each example in the batch\n",
    "    for b in range(B):\n",
    "        # loop through each output channel\n",
    "        for c_out in range(C_out):\n",
    "            # loop through each output location\n",
    "            for i in range(0, H_out):\n",
    "                for j in range(0, W_out):\n",
    "                    reconstructed_input[b, :, i*stride[0]:i*stride[0]+kH, j*stride[1]:j*stride[1]+kW] += torch.mul(weights[c_out, :, :, :], activations[b, c_out, i, j])\n",
    "    if padding[0] == 0 and padding[1] == 0:\n",
    "        return reconstructed_input\n",
    "    else:\n",
    "        return reconstructed_input[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n",
    "\n",
    "def winner_takes_all(activations):\n",
    "    # Get the maximum activation value and its index along the channel axis (C_out)\n",
    "    max_value, max_index = torch.max(activations, dim=1)\n",
    "    # Create a tensor of the same shape as the activations, but with all elements set to zero\n",
    "    zero_tensor = torch.zeros_like(activations)\n",
    "    # Use the max_index to set the corresponding activation to the max_value, and zero out the other activations\n",
    "    for b in range(activations.shape[0]):\n",
    "        for h in range(activations.shape[2]):\n",
    "            for w in range(activations.shape[3]):\n",
    "                zero_tensor[b, max_index[b, h, w], h, w] = max_value[b, h, w]\n",
    "    return zero_tensor\n",
    "\n",
    "def top_n_activations(tensor, Perc):\n",
    "    \"\"\"\n",
    "    tensor: tensor of shape batch, channel, h, w \n",
    "    Perc: top % of activations to keep in tensor, the rest will be set to zero\n",
    "    \"\"\"\n",
    "    batch, channel, h, w = tensor.shape\n",
    "    flat_tensor = tensor.reshape(batch, channel, h * w)\n",
    "    N = int(float(Perc) * h * w // 100)\n",
    "    print(f'{N} source neurons out of {h * w}')\n",
    "    top_values, top_indices = torch.topk(flat_tensor, N, dim=2)\n",
    "    mask = torch.zeros_like(flat_tensor)\n",
    "    mask.scatter_(2, top_indices, 1)\n",
    "    mask = mask.view(batch, channel, h, w)\n",
    "    result = tensor * mask\n",
    "    print(\"Number of activations:\",torch.count_nonzero(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "960ed1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of a mess, clean later\n",
    "def de_normalize(tensor, mean, std):\n",
    "    for i, (m, s) in enumerate(zip(mean, std)):\n",
    "        tensor[i] = tensor[i] * s + m\n",
    "    return tensor\n",
    "# def de_normalize(tensor, mean, std):\n",
    "#     for i, (m, s) in enumerate(zip(mean, std)):\n",
    "#         tensor[i] = tensor[i] * torch.tensor(s, device=tensor.device) + torch.tensor(m, device=tensor.device)\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "def reverse_transform(img_tensor):\n",
    "    # Step 1: De-normalize\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    de_normalized_tensor = de_normalize(img_tensor.clone().squeeze(), mean, std)\n",
    "    img = F2.to_pil_image(de_normalized_tensor)\n",
    "    #Cropping is irreversible\n",
    "    return img\n",
    "\n",
    "def normalize_image_norescale(image):\n",
    "    \"\"\"\n",
    "    Normalize only\n",
    "    \"\"\"\n",
    "    norm = (image - image.mean())/image.std()\n",
    "    return norm\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalize and Rescale\n",
    "    \"\"\"\n",
    "    norm = (image - image.mean())/image.std()\n",
    "    norm = norm * 0.1\n",
    "    norm = norm + 0.5\n",
    "    norm = norm.clip(0, 1)\n",
    "    return norm\n",
    "\n",
    "\n",
    "def normalize_image_minmax_norescale(image):\n",
    "    \"\"\"\n",
    "    Normalize using min/max and Rescale\n",
    "    \"\"\"\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    return image\n",
    "\n",
    "def normalize(X, normalization_method=\"normalize_std\", new_min=-1, new_max=1):\n",
    "    if X.dim() == 4:  # Conv layer (batch, channel, h, w)\n",
    "        axis = (2, 3)\n",
    "    elif X.dim() == 2:  # Dense layer (batch, N)\n",
    "        axis = 1\n",
    "    else:\n",
    "        raise ValueError(\"Input tensor must have 2 or 4 dimensions\")\n",
    "    if normalization_method == \"normalize_std\": #Local normalization only\n",
    "        X -= X.mean(axis, keepdim=True)\n",
    "        X /= X.std(axis, keepdim=True)\n",
    "    elif  normalization_method == \"normalize_std_global\":\n",
    "        X = (X - X.mean()) / X.std()\n",
    "    elif normalization_method == \"normalize_rescale_std\":\n",
    "        X = (X - X.mean()) / X.std()\n",
    "        X = X * 0.1 + 0.5\n",
    "        X = X.clip(0, 1)\n",
    "    elif normalization_method == \"normalize_rescale_std_local\":\n",
    "        X -= X.mean(axis, keepdim=True)\n",
    "        X /= X.std(axis, keepdim=True)\n",
    "        X = X * 0.1 + 0.5\n",
    "        X = X.clip(0, 1)\n",
    "    elif normalization_method == \"normalize_minmax_rescale\":\n",
    "        X = (X - X.min()) / (X.max() - X.min())\n",
    "        X = X * (new_max - new_min) + new_min\n",
    "        X = X.clip(new_min, new_max)\n",
    "    elif normalization_method == \"normalize_minmax\":\n",
    "        X = (X - X.min()) / (X.max() - X.min())\n",
    "    elif normalization_method == \"normalize_minmax_local\":\n",
    "        if axis == 1:\n",
    "            X_min =  X.min(dim = axis, keepdim=True).values\n",
    "            X_max =  X.max(dim = axis, keepdim=True).values\n",
    "        else:\n",
    "            X_min=X\n",
    "            X_max=X\n",
    "            for dim in axis:\n",
    "                X_min =  X_min.min(dim = dim, keepdim=True).values\n",
    "                X_max =  X_max.max(dim = dim, keepdim=True).values                \n",
    "        X = (X -X_min) / ( X_max - X_min)\n",
    "    elif normalization_method == \"normalize_minmax_01_05_rescale\":\n",
    "        X = (X - X.min()) / (X.max() - X.min())\n",
    "        X = X * 0.1 + 0.5\n",
    "        X = X.clip(0, 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization method\")\n",
    "    X = fix_nan(X)\n",
    "    return X\n",
    "\n",
    "def activation(X, normalization_method=\"thresh_00\"):\n",
    "    if normalization_method == \"thresh_00\": #Local normalization only\n",
    "        X[X < 0] = 0\n",
    "    elif normalization_method == \"thresh_01\":\n",
    "        X[X < 0.1] = 0\n",
    "    elif normalization_method == \"thresh_02\":\n",
    "        X[X < 0.2] = 0\n",
    "    elif normalization_method == \"thresh_03\":\n",
    "        X[X < 0.3] = 0\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization method\")\n",
    "    X = fix_nan(X)\n",
    "    return X\n",
    "\n",
    "def fix_nan(reconstructed_input):\n",
    "    nan_inf_exist = torch.isnan(reconstructed_input).any() or torch.isinf(reconstructed_input).any()\n",
    "    # Replace NaN values with a small positive number epsilon\n",
    "    reconstructed_input = torch.where(torch.isnan(reconstructed_input), torch.tensor(epsilon).to(device), reconstructed_input)\n",
    "    # Replace positive infinity with a large positive number\n",
    "    pos_inf_mask = torch.isinf(reconstructed_input) & (reconstructed_input > 0)\n",
    "    reconstructed_input = torch.where(pos_inf_mask, torch.tensor(large_n).to(device), reconstructed_input)\n",
    "    # Replace negative infinity with a large negative number\n",
    "    neg_inf_mask = torch.isinf(reconstructed_input) & (reconstructed_input < 0)\n",
    "    reconstructed_input = torch.where(neg_inf_mask, torch.tensor(large_n).to(device), reconstructed_input)\n",
    "    nan_inf_exist = torch.isnan(reconstructed_input).any() or torch.isinf(reconstructed_input).any()\n",
    "    return reconstructed_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0f96c",
   "metadata": {},
   "source": [
    "## `Dashboard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b47e046b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d8d43d4ae5476aad9c8a5b50474a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Image:', options=('rot.png', 'Upload your own file', 'cat.jpg', 'Cat_word.png', 'cat_rec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec2b1074c09401c9e6695269aa49770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Reconstruction:', options=('last_layer', 'general_class', 'random_neuron', 'source1', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1287413c86154104aece47e79b22ff44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Take top activations')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fb3b1cfd6a4816b82f27fd97085c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='top percent', layout=Layout(display='none'), style=DescriptionStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2c711cb95841cc9590240b40ecee39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Positivise')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d76b09f5c0242199b9de77f88dc3b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Positivise_conv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615a301228b2410ba526f9e1a94975b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Normalize')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28874e1a01f4cbb9eb24c98961fc9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Winner Takes All:', options=(0, 1), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093b34ac315a4e1bb268634c296c3e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Take Positive')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be5d8a01bbd4ef78a5b1e5b9b7aa6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Normalization Method:', options=('normalize_rescale_std', 'normalize_std', 'normalize_mi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d41af0e570f4f83bcba550dc0d658e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Tensor visualization Method:', options=('reverse_transform', 'normalize', 'normalize_ima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4578a3e41f6411ca02036f225349927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='image/*', description='Upload Photo', layout=Layout(visibility='hidden'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294029b2fc6141c2a2af4fef94d1c1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066cc08253a44dec81f9578c8dcef482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reconstruct Image', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FileUpload\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "\n",
    "def reconstruct_image(im_class, reconstruction_method, Positivise, Positivise_conv, Normalize, does_winner_take_all, take_positive, normalization_method, tensor_viz , is_top, top_perc):\n",
    "    # Load pretrained AlexNet model\n",
    "    model = modified_alexnet #models.alexnet(pretrained=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    # Check if an image is uploaded\n",
    "    if im_class == 'Upload your own file':\n",
    "        if upload_widget.value:\n",
    "            uploaded_image = list(upload_widget.value.values())[0]\n",
    "            image_content = uploaded_image['content']\n",
    "            image = Image.open(BytesIO(image_content)).convert('RGB')\n",
    "            #upload_widget.value.clear() # clear the uploaded image after processing\n",
    "        else:\n",
    "            display('No File has been uploaded', upload_widget.value)\n",
    "            return\n",
    "    #elif 'rot' in im_class:\n",
    "    #    image = Image.open(f'./{im_class}.png').convert('RGB')\n",
    "    elif im_class not in set([str(i) for i in random_mnist_images.keys()]):\n",
    "        image = Image.open(f'./{im_class}').convert('RGB')\n",
    "    else:\n",
    "        image = random_mnist_images[int(im_class)]\n",
    "        image = (image * 0.5) + 0.5\n",
    "        # Convert the PyTorch tensor to a PIL Image for display\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        # Convert the PIL Image to RGB format\n",
    "        image = image.convert('RGB')\n",
    "    img_tensor = transform(image).unsqueeze(0).requires_grad_()\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    # Extract all weights and biases\n",
    "    weights = [p.data for p in model.parameters()]\n",
    "    # Define activation functions\n",
    "    relu = torch.nn.ReLU(inplace=True)\n",
    "    # Define layer-by-layer operations\n",
    "    x = img_tensor.detach()\n",
    "    # Convolution 1\n",
    "    w1, b1 = weights[0], weights[1]\n",
    "    #(0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "    x = torch.nn.functional.conv2d(x, w1, b1, stride=4, padding=2)\n",
    "    c1 = x.detach()\n",
    "    x = relu(x)\n",
    "    c1rel = x.detach()\n",
    "    # Old MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    # Replaced by strided conv that was slightly retrained on Cifar \n",
    "    w1p, b1p = weights[2], weights[3]\n",
    "    x = torch.nn.functional.conv2d(x, w1p, b1p, stride=2, padding=0)\n",
    "    c1relmp = x.detach()\n",
    "\n",
    "    # Convolution 2\n",
    "    w2, b2 = weights[4], weights[5]\n",
    "    #(3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    x = torch.nn.functional.conv2d(x, w2, b2, stride=1, padding=2)\n",
    "    c2=x.detach()\n",
    "    x = relu(x)\n",
    "    c2rel=x.detach()\n",
    "    #Old (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    #replaced by conv:\n",
    "    w2p, b2p = weights[6], weights[7]\n",
    "    x = torch.nn.functional.conv2d(x, w2p, b2p, stride=2, padding=0)\n",
    "    c2relmp = x.detach()\n",
    "\n",
    "    # Convolution 3\n",
    "    w3, b3 = weights[8], weights[9]\n",
    "    #(6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  \n",
    "    x = torch.nn.functional.conv2d(x, w3, b3, stride=1, padding=1)\n",
    "    x = relu(x)\n",
    "    c3rel = x.detach()\n",
    "\n",
    "    # Convolution 4\n",
    "    w4, b4 = weights[10], weights[11]\n",
    "    #(8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    x = torch.nn.functional.conv2d(x, w4, b4, stride=1, padding=1)\n",
    "    x = relu(x)\n",
    "    c4rel = x.detach()\n",
    "\n",
    "    # Convolution 5\n",
    "    w5, b5 = weights[12], weights[13]\n",
    "    #(10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    x = torch.nn.functional.conv2d(x, w5, b5, stride=1, padding=1)\n",
    "    c5 = x.detach()\n",
    "    x = relu(x)\n",
    "    c5rel = x.detach()\n",
    "    #(12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    #replaced by new Conv\n",
    "    w5p, b5p = weights[14], weights[15]\n",
    "    x = torch.nn.functional.conv2d(x, w5p, b5p, stride=2, padding=0)\n",
    "    c5relmp = x.detach()\n",
    "    x = torch.nn.functional.adaptive_avg_pool2d(x,output_size=(6, 6))\n",
    "\n",
    "    # Fully connected 1\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    w6, b6 = weights[16], weights[17]\n",
    "    x = torch.nn.functional.linear(x, w6, b6)\n",
    "    x = relu(x)\n",
    "    fc1rel = x.detach()\n",
    "\n",
    "    # Fully connected 2\n",
    "    w7, b7 = weights[18], weights[19]\n",
    "    x = torch.nn.functional.linear(x, w7, b7)\n",
    "    fc2 = x.detach()\n",
    "    x = relu(x)\n",
    "    fc2rel = x.detach()\n",
    "\n",
    "    # Fully connected 3\n",
    "    w8, b8 = weights[20], weights[21]\n",
    "    x = torch.nn.functional.linear(x, w8, b8)\n",
    "    x = relu(x)\n",
    "    fc3rel = x.detach()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = model(img_tensor)\n",
    "        pred_class = output.argmax().item() \n",
    "        print(pred_class)\n",
    "    #assert torch.equal(x,output)\n",
    "    if reconstruction_method == \"general_class\":\n",
    "        target_class = pred_class # e.g. let's do the backpropagation of the resulting class in general \n",
    "        source_pointer_neurons = torch.zeros(output.shape, dtype=torch.float)\n",
    "        source_pointer_neurons[0][target_class] = 1\n",
    "        source_pointer_neurons = source_pointer_neurons.to(device)\n",
    "        # Fully connected 3 \n",
    "        reconstructed_input = reconstructed_input_dense_simple(w8, source_pointer_neurons, take_positive)\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "        if Normalize:\n",
    "            reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "        if Positivise:\n",
    "            reconstructed_input[reconstructed_input < 0] = 0\n",
    "    elif reconstruction_method == \"random_neuron\":\n",
    "        while True:\n",
    "            target_class = random.randint(1,999) # 1000 classes \n",
    "            if target_class != pred_class:\n",
    "                break\n",
    "        print('reconstructing class:',target_class)\n",
    "        source_pointer_neurons = torch.zeros(output.shape, dtype=torch.float)\n",
    "        source_pointer_neurons[0][target_class] = 1\n",
    "        source_pointer_neurons = source_pointer_neurons.to(device)\n",
    "        # Fully connected 3 \n",
    "        reconstructed_input = reconstructed_input_dense_simple(w8, source_pointer_neurons, take_positive)\n",
    "        min_v = torch.min(reconstructed_input)\n",
    "        max_v = torch.max(reconstructed_input)\n",
    "        reconstructed_input = (max_v - min_v) * torch.rand(1,4096) + min_v\n",
    "        reconstructed_input = fix_nan(reconstructed_input)\n",
    "        if Normalize:\n",
    "            reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "        if Positivise:\n",
    "            reconstructed_input[reconstructed_input < 0] = 0\n",
    "    elif reconstruction_method == \"last_layer\":\n",
    "        reconstructed_input = fc3rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "        if Normalize:\n",
    "            reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "        if Positivise: # redundant with the top selection\n",
    "            reconstructed_input[reconstructed_input < 0] = 0\n",
    "        # Fully connected 3\n",
    "        reconstructed_input = reconstructed_input_dense_simple(w8, reconstructed_input, take_positive)\n",
    "        if Normalize:\n",
    "            reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "        if Positivise: # redundant with the top selection\n",
    "            reconstructed_input[reconstructed_input < 0] = 0\n",
    "    else:# the rest of the reconstruction methods (source1 till sourceX) Dirty implementation, but i'm in a hurry, hein! trying to finish before GPT5 :p\n",
    "        reconstructed_input = torch.randn_like(fc2rel)\n",
    "    if reconstruction_method == \"source1\":\n",
    "        reconstructed_input = fc2rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise: # redundant with the top selection\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    # Reverse Fully connected 2\n",
    "    reconstructed_input = reconstructed_input_dense_simple(w7, reconstructed_input, take_positive)\n",
    "    if reconstruction_method == \"source2\":\n",
    "        reconstructed_input = fc1rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise: # redundant with the top selection\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    # Fully connected 1\n",
    "    reconstructed_input = reconstructed_input_dense_simple(w6, reconstructed_input, take_positive)\n",
    "    if reconstruction_method == \"source3\":\n",
    "        reconstructed_input = c5relmp.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    #Adaptive avg pooing reversed:\n",
    "    reconstructed_input = reconstructed_input\n",
    "    ## Backward for conv replacement of pool\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input.reshape(1,256,6,6), w5p, stride=(2,2), padding=(0,0), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source4\":\n",
    "        reconstructed_input = c5rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    ## Backward for conv layers\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w5, stride=(1,1), padding=(1,1), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source5\":\n",
    "        reconstructed_input = c4rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w4, stride=(1,1), padding=(1,1), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source6\":\n",
    "        reconstructed_input = c3rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w3, stride=(1,1), padding=(1,1), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source7\":\n",
    "        reconstructed_input = c2relmp.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    if is_top and reconstruction_method == \"source7\":\n",
    "        reconstructed_input = top_n_activations(reconstructed_input, top_perc)\n",
    "    ## Backward for conv replacement of pool\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w2p, stride=(2,2), padding=(0,0), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source8\":\n",
    "        reconstructed_input = c2rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    if is_top and reconstruction_method == \"source8\": \n",
    "        reconstructed_input = top_n_activations(reconstructed_input, top_perc)\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w2, stride=(1,1), padding=(2,2), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source9\":\n",
    "        reconstructed_input = c1relmp.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    if is_top and reconstruction_method == \"source9\":\n",
    "        reconstructed_input = top_n_activations(reconstructed_input, top_perc)\n",
    "    ## Backward for conv replacement of pool\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w1p, stride=(2,2), padding=(0,0), does_winner_take_all = does_winner_take_all)\n",
    "    if reconstruction_method == \"source10\":\n",
    "        reconstructed_input = c1rel.detach()\n",
    "        reconstructed_input = reconstructed_input.to(device)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0\n",
    "    if is_top and reconstruction_method == \"source10\":\n",
    "        reconstructed_input = top_n_activations(reconstructed_input, top_perc)\n",
    "    reconstructed_input = reconstructed_input_conv2D_correct(reconstructed_input, w1, stride=(4,4), padding=(2,2), does_winner_take_all = does_winner_take_all)\n",
    "    if Normalize:\n",
    "        reconstructed_input = normalize(reconstructed_input, normalization_method=normalization_method)\n",
    "    if Positivise_conv:\n",
    "        reconstructed_input[reconstructed_input < 0] = 0 \n",
    "    #back to CPU to finish \n",
    "    reconstructed_input = reconstructed_input.cpu()\n",
    "    reconstructed_image = reconstructed_input[0].permute(1,2,0)\n",
    "    if tensor_viz == 'normalize':\n",
    "        reconstructed_image = normalize_image(reconstructed_image)\n",
    "    elif tensor_viz == 'normalize_std_norescale':\n",
    "        reconstructed_image = normalize_image_norescale(reconstructed_image)\n",
    "    elif tensor_viz == 'reverse_transform':\n",
    "        reconstructed_image = reverse_transform(reconstructed_input[0].detach())#may be reconstructed_image.detach().numpy()\n",
    "    elif tensor_viz =='normalize_image_minmax_norescale':\n",
    "        reconstructed_image = normalize_image_minmax_norescale(reconstructed_image)\n",
    "    # Save and display the images\n",
    "    output_dir = f'{output_images_dir}/{im_class[:-4]}/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    original_image_path = os.path.join(output_dir, f'{im_class}_original.png')\n",
    "    reconstructed_image_path = os.path.join(output_dir, f\"{im_class}_{reconstruction_method}_{'Pos' if Positivise else 'Neg'}_{'PosConv' if Positivise_conv else 'NegConv'}_{'Norm' if Normalize else 'Unnorm'}_{does_winner_take_all}_{'TakePos' if take_positive else 'NoTakePos'}_{normalization_method}_{tensor_viz}__{is_top}_{top_perc}.png\")\n",
    "    if tensor_viz == 'normalize' or tensor_viz == 'normalize_image_minmax_norescale' or tensor_viz == 'normalize_std_norescale':\n",
    "        reconstructed_image_pil = Image.fromarray((reconstructed_image.cpu().numpy() * 255).astype(np.uint8))\n",
    "        reconstructed_image_pil.save(reconstructed_image_path)\n",
    "    else:\n",
    "        reconstructed_image.save(reconstructed_image_path)\n",
    "    resized_original_image = reverse_transform(img_tensor)\n",
    "    resized_original_image.save(original_image_path)\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(resized_original_image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(reconstructed_image)\n",
    "    ax[1].set_title('Reconstructed Image')\n",
    "    plt.show()\n",
    "    return reconstructed_input\n",
    "\n",
    "# Create widgets\n",
    "im_class_widget = widgets.Dropdown(options=['rot.png','Upload your own file', 'cat.jpg','Cat_word.png','cat_reconstruct.jpg','Berkeley.jpg','Leibniz.jpg','zied.jpg','5', '0', '4', '1', '9', '2', '3', '6', '7', '8'], description='Image:')\n",
    "reconstruction_method_widget = widgets.Dropdown(options=['last_layer','general_class','random_neuron','source1','source2','source3','source4','source5','source6','source7','source8','source9','source10'], description='Reconstruction:')\n",
    "Positivise_widget = widgets.Checkbox(value=True, description='Positivise')\n",
    "Positivise_conv_widget = widgets.Checkbox(value=True, description='Positivise_conv')\n",
    "Normalize_widget = widgets.Checkbox(value=True, description='Normalize')\n",
    "does_winner_take_all_widget = widgets.Dropdown(options=[0, 1], description='Winner Takes All:')\n",
    "take_positive_widget = widgets.Checkbox(value=False, description='Take Positive')#Should be deprecated, don't have time :/\n",
    "normalization_method_widget = widgets.Dropdown(options=['normalize_rescale_std','normalize_std', 'normalize_minmax_rescale','normalize_std_global', 'normalize_rescale_std_local','normalize_minmax_01_05_rescale','normalize_minmax_local', 'normalize_minmax'], description='Normalization Method:')\n",
    "#activation_method_widget = widgets.Dropdown(options=['thresh_00','thresh_01','thresh_02','thresh_03'], description='Activation Method:')# \n",
    "tensor_visualization_widget = widgets.Dropdown(options=['reverse_transform','normalize','normalize_image_minmax_norescale','normalize_std_norescale'], description='Tensor visualization Method:')\n",
    "upload_widget = FileUpload(description='Upload Photo', accept='image/*', multiple=False)\n",
    "upload_widget.layout.visibility = 'hidden' # set initial visibility to 'hidden'\n",
    "is_top_checkbox_widget = widgets.Checkbox(value=False, description='Take top activations')\n",
    "top_percent_textbox_widget = widgets.Text(description='top percent', style={'description_width': 'initial'}) # Initially not visible\n",
    "\n",
    "############\"\n",
    "# Function to toggle visibility of the textbox based on checkbox\n",
    "def toggle_textbox(change):\n",
    "    if change['new']:  # If checkbox is checked\n",
    "        top_percent_textbox_widget.layout.display = 'flex'  # Show textbox\n",
    "    else:\n",
    "        top_percent_textbox_widget.layout.display = 'none'  # Hide textbox\n",
    "\n",
    "# Linking the checkbox state to the function\n",
    "is_top_checkbox_widget.observe(toggle_textbox, names='value')\n",
    "\n",
    "# Set initial display state of the textbox\n",
    "top_percent_textbox_widget.layout.display = 'none'\n",
    "\n",
    "# Create button\n",
    "run_button = widgets.Button(description='Reconstruct Image')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_im_class_change(change):\n",
    "    if change['new'] == 'Upload your own file':\n",
    "        upload_widget.layout.visibility = 'visible'\n",
    "    else:\n",
    "        upload_widget.layout.visibility = 'hidden'\n",
    "\n",
    "im_class_widget.observe(on_im_class_change, names='value')\n",
    "\n",
    "def on_upload_button_click(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value' and change['new'] is not None:\n",
    "        im_class_widget.value = 'Upload your own file'\n",
    "        #upload_widget.value.clear()\n",
    "\n",
    "upload_widget.observe(on_upload_button_click)\n",
    "\n",
    "\n",
    "def on_button_click(button):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        output_dir = f'{output_images_dir}/{im_class_widget.value[:-4]}/'\n",
    "        # Generate the filename based on the current widget values\n",
    "        filename = f\"{im_class_widget.value}_{reconstruction_method_widget.value}_{'Pos' if Positivise_widget.value else 'Neg'}_{'PosConv' if Positivise_conv_widget.value else 'NegConv'}_{'Norm' if Normalize_widget.value else 'Unnorm'}_{does_winner_take_all_widget.value}_{'TakePos' if take_positive_widget.value else 'NoTakePos'}_{normalization_method_widget.value}_{tensor_visualization_widget.value}_{is_top_checkbox_widget.value}_{top_percent_textbox_widget.value}.png\"\n",
    "        reconstructed_image_path = os.path.join(output_dir, filename)\n",
    "        # Check if the image with this filename already exists:\n",
    "        if os.path.exists(reconstructed_image_path):\n",
    "            resized_original_image = Image.open(os.path.join(output_dir, f'{im_class_widget.value}_original.png'))\n",
    "            reconstructed_image = Image.open(reconstructed_image_path)\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(resized_original_image)\n",
    "            ax[0].set_title('Original Image')\n",
    "            ax[1].imshow(reconstructed_image)\n",
    "            ax[1].set_title('Reconstructed Image')\n",
    "            plt.show()\n",
    "        else:\n",
    "            reconstruct_image(\n",
    "                im_class_widget.value,\n",
    "                reconstruction_method_widget.value,\n",
    "                Positivise_widget.value,\n",
    "                Positivise_conv_widget.value,\n",
    "                Normalize_widget.value,\n",
    "                does_winner_take_all_widget.value,\n",
    "                take_positive_widget.value,\n",
    "                normalization_method_widget.value,\n",
    "                tensor_visualization_widget.value,\n",
    "                is_top_checkbox_widget.value,\n",
    "                top_percent_textbox_widget.value\n",
    "                )\n",
    "            \n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(im_class_widget)\n",
    "display(reconstruction_method_widget)\n",
    "display(is_top_checkbox_widget)\n",
    "display(top_percent_textbox_widget)\n",
    "display(Positivise_widget)\n",
    "display(Positivise_conv_widget)\n",
    "display(Normalize_widget)\n",
    "display(does_winner_take_all_widget)\n",
    "display(take_positive_widget)\n",
    "display(normalization_method_widget)\n",
    "display(tensor_visualization_widget)\n",
    "display(upload_widget)\n",
    "display(output)\n",
    "display(run_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce207d-e490-4751-953a-2dd7929d1280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:elias]",
   "language": "python",
   "name": "conda-env-elias-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
